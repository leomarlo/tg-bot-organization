services:
  tg-bot:
    image: ghcr.io/leomarlo/tg-bot-organization:latest
    container_name: tg-bot-italian
    env_file: [.env]
    environment:
      - MODE=webhook
      - PORT=8000
      - LLM_SERVICE_URL=http://llm-service:8001
      - LLM_TIMEOUT_S=10
    volumes:
      - ./data:/app/src/tg_bot_italian/data
    restart: unless-stopped
    expose:
      - "8000"

  llm-service:
    image: ghcr.io/leomarlo/tg-bot-organization:latest
    container_name: llm-service
    env_file: [.env]
    environment:
      - LLM_PROVIDER=ollama
      - OLLAMA_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3.2:1b
      - LLM_TIMEOUT_S=30
    command: ["poetry", "run", "uvicorn", "llm_service.main:api", "--host", "0.0.0.0", "--port", "8001"]
    restart: unless-stopped
    expose:
      - "8001"
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    expose:
      - "11434"

  caddy:
    image: caddy:2
    container_name: caddy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    restart: unless-stopped

volumes:
  caddy_data:
  caddy_config:
  ollama_data:
